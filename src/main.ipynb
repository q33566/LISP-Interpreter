{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(+)\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case_input = list(Path('../public_test_data').iterdir())\n",
    "test_case_input[0].read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken((,'(',1,0)\n",
      "LexToken(+,'+',1,2)\n",
      "LexToken(NUMBER,'1',1,4)\n",
      "LexToken(NUMBER,'2',1,6)\n",
      "LexToken(),')',1,8)\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "from ply import lex\n",
    "import tokenizer\n",
    "from pathlib import Path\n",
    "lexer = lex.lex(module = tokenizer)\n",
    "lexer.input(input('input: '))\n",
    "# Tokenize\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok:\n",
    "        break      # No more input\n",
    "    print(tok)\n",
    "print('finish')\n",
    "# test_data_path = Path('../public_test_data')\n",
    "# for file in test_data_path.iterdir():\n",
    "#     content = file.read_text(encoding='utf-8')\n",
    "#     print(content)\n",
    "#     # Give the lexer some input\n",
    "#     lexer.input(content)\n",
    "#     # Tokenize\n",
    "#     while True:\n",
    "#         tok = lexer.token()\n",
    "#         if not tok:\n",
    "#             break      # No more input\n",
    "#         print(tok)\n",
    "#     print('finish')\n",
    "# lexer.input(\"\"\"\n",
    "# (define foo)\n",
    "# \"\"\")\n",
    "# while True:\n",
    "#     tok = lexer.token()\n",
    "#     if not tok:\n",
    "#         break      # No more input\n",
    "#     print(tok)\n",
    "# print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = 4\n",
    "a.append(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'__file__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyntax error in input!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Build the parser\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43myacc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myacc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m    \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\NCU\\Courses\\Junior\\Compiler\\LISP-Interpreter\\.venv\\lib\\site-packages\\ply\\yacc.py:3256\u001b[0m, in \u001b[0;36myacc\u001b[1;34m(method, debug, module, tabmodule, start, check_recursion, optimize, write_tables, debugfile, outputdir, debuglog, errorlog, picklefile)\u001b[0m\n\u001b[0;32m   3254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tabmodule:\n\u001b[1;32m-> 3256\u001b[0m         srcfile \u001b[38;5;241m=\u001b[39m \u001b[43mpdict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__file__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3258\u001b[0m         parts \u001b[38;5;241m=\u001b[39m tabmodule\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: '__file__'"
     ]
    }
   ],
   "source": [
    "import ply.yacc as yacc\n",
    "from tokenizer import tokens\n",
    "\n",
    "def p_program(p):\n",
    "    'program: stmts'\n",
    "def p_stmts(p):\n",
    "    'stmts: stmt stmts'\n",
    "    '     | empty'\n",
    "    \n",
    "def p_exps(p):\n",
    "    'exps: exp exps'\n",
    "    '    | empty'\n",
    "def p_exp(p):\n",
    "    'exp: BOOL_VAL'\n",
    "    '   | NUMBER'\n",
    "    '   | variable'\n",
    "    '   | num_op'\n",
    "    '   | logical_op'\n",
    "    '   | fun_exp'\n",
    "    '   | fun_call'\n",
    "    '   | if_exp'\n",
    "\n",
    "def p_num_op(p):\n",
    "    'num_op     : plus'\n",
    "    '           | minus'\n",
    "    '           | multiply'\n",
    "    '           | divide'\n",
    "    '           | modulus'\n",
    "    '           | greater'\n",
    "    '           | smaller'\n",
    "    '           | equal'\n",
    "    'plus       : (+ exp exps)'\n",
    "    'minus      : (- exp exps)'\n",
    "    'multiply   : (* exp exps)'\n",
    "    'divide     : (/ exp exps)'\n",
    "    'modulus    : (mod exp exps)'\n",
    "    'greater    : (> exp exps)'\n",
    "    'smaller    : (< exp exps)'\n",
    "    'equal      : (= exp exps)'\n",
    "\n",
    "def p_ligical_op(p):\n",
    "    'logical_op: and_op'\n",
    "    '          | or_op'\n",
    "    '          | not_op'\n",
    "    'and_op:    (AND exp exps)'\n",
    "    'or_op:    (OR exp exps)'\n",
    "    'not_op:    (NOT exp)'\n",
    "\n",
    "def p_def_stmt(p):\n",
    "    'DEFINE variable exp'\n",
    "    'variable: id'\n",
    "\n",
    "def p_fun_exp(p):\n",
    "    'fun_exp: (FUN fun_ids fun_body)'\n",
    "    'fun_ids: (ids)'\n",
    "    'ids: ID ids'\n",
    "    '   | empty'\n",
    "    'fun_body: exp'\n",
    "    'params: param params'\n",
    "    '      | empty'\n",
    "    'fun_call: fun_exp params'\n",
    "    '        | fun_name params'\n",
    "    'param: exp'\n",
    "    'last_exp: exp'\n",
    "    'fun_name: id'\n",
    "def p_if_exp(p):\n",
    "    'if_exp: (IF test_exp then_exp else_exp)'\n",
    "    'test_exp: exp'\n",
    "    'then_exp: exp'\n",
    "    'else_exp: exp'\n",
    "    \n",
    "# Error rule for syntax errors\n",
    "def p_error(p):\n",
    "    print(\"Syntax error in input!\")\n",
    "\n",
    "# Build the parser\n",
    "parser = yacc.yacc()\n",
    "\n",
    "while True:\n",
    "   try:\n",
    "       inp = ('(+)')\n",
    "       lexer = lex.lex(module=tokenizer)\n",
    "       lexer.input(inp)\n",
    "       s = input(inp)\n",
    "   except EOFError:\n",
    "       break\n",
    "   if not s: continue\n",
    "   result = parser.parse(s)\n",
    "   print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
